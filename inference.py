# inference.py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import readline  # readlineã¯ã€input()ã§ä¸Šä¸‹çŸ¢å°ã‚­ãƒ¼ã«ã‚ˆã‚‹å±¥æ­´å‚ç…§ã‚’å¯èƒ½ã«ã™ã‚‹ä¾¿åˆ©ãªãƒ„ãƒ¼ãƒ«ã§ã™

# --- 1. ã™ã¹ã¦ã®ãƒ‘ã‚¹ã¨ãƒ‡ãƒã‚¤ã‚¹ã‚’å®šç¾© ---
# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã€‚æœ€åˆã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸQwen2ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡ã—ã¾ã™
base_model_path = "./Qwen/Qwen2___5-Coder-7B-Instruct"

# ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒ‘ã‚¹ã€‚æ­£å¸¸ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„
adapter_path = "./results_qwen2_coder_sft_guide/final_checkpoint"

# ä½¿ç”¨ã™ã‚‹ãƒ¡ã‚¤ãƒ³GPUã‚’æŒ‡å®š
main_gpu_device = "cuda:0"

# --- 2. 4ãƒ“ãƒƒãƒˆé‡å­åŒ–è¨­å®šã‚’ãƒ­ãƒ¼ãƒ‰ ---
print(">> ã‚¹ãƒ†ãƒƒãƒ—1/4: 4ãƒ“ãƒƒãƒˆé‡å­åŒ–è¨­å®šã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16  # è¨ˆç®—æ™‚ã«bf16ã‚’ä½¿ç”¨ã—ã¦è‰¯å¥½ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¾—ã‚‹
)

# --- 3. 4ãƒ“ãƒƒãƒˆã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ãƒ­ãƒ¼ãƒ‰ ---
print(f">> ã‚¹ãƒ†ãƒƒãƒ—2/4: '{base_model_path}' ã‹ã‚‰4ãƒ“ãƒƒãƒˆã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    quantization_config=bnb_config,
    device_map=main_gpu_device,
    trust_remote_code=True,
)
tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®š

# --- 4. LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒãƒ¼ã‚¸ ---
print(f">> ã‚¹ãƒ†ãƒƒãƒ—3/4: LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ '{adapter_path}' ã‚’ãƒ¢ãƒ‡ãƒ«ã«é©ç”¨ä¸­...")
# PeftModelã¯ã€é‡å­åŒ–ã•ã‚ŒãŸãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ä¸Šã«ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’è‡ªå‹•çš„ã‹ã¤åŠ¹ç‡çš„ã«é©ç”¨ã—ã¾ã™
model = PeftModel.from_pretrained(base_model, adapter_path)
model = model.eval()  # å¿…ãšè©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™

print("\nâœ… ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã€æº–å‚™ãŒæ•´ã„ã¾ã—ãŸï¼è³ªå•ã‚’é–‹å§‹ã§ãã¾ã™ã€‚")
print("   'exit' ã¾ãŸã¯ 'quit' ã¨å…¥åŠ›ã—ã¦ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
print("=" * 50)

# --- 5. å¯¾è©±çš„ãªæ¨è«–ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ ---
while True:
    try:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’å–å¾—
        instruction = input("æŒ‡ç¤ºã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (ä¾‹: ãƒ´ã‚£ãƒªãƒ‡ã‚£ã‚¢è«¸å³¶ã®é¦–éƒ½ã¯ã©ã“ã§ã™ã‹ï¼Ÿ): \n> ")
        if instruction.lower() in ["exit", "quit"]:
            break
        if not instruction:
            continue

        # ä¸­æ ¸ã¨ãªã‚‹ã‚¹ãƒ†ãƒƒãƒ—: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚ã¨å®Œå…¨ã«åŒã˜å…¬å¼å¯¾è©±ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦å…¥åŠ›ã‚’æ§‹ç¯‰ã™ã‚‹
        # ã“ã‚Œã«ã‚ˆã‚Šã€"<|im_start|>user\næŒ‡ç¤º<|im_end|>\n<|im_start|>assistant\n" ã®ã‚ˆã†ãªå½¢å¼ãŒç”Ÿæˆã•ã‚Œã¾ã™
        messages = [
            {"role": "user", "content": instruction}
        ]
        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

        inputs = tokenizer(prompt, return_tensors="pt").to(main_gpu_device)

        print("\nğŸ¤” ãƒ¢ãƒ‡ãƒ«ãŒå›ç­”ã‚’ç”Ÿæˆä¸­...")

        # ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã‚’ç”Ÿæˆ
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,  # å¿…è¦ã«å¿œã˜ã¦ç”Ÿæˆã™ã‚‹é•·ã•ã‚’èª¿æ•´ã§ãã¾ã™
            pad_token_id=tokenizer.eos_token_id,
            do_sample=True,  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’æœ‰åŠ¹ã«ã—ã¦ã€å›ç­”ã‚’ã‚ˆã‚Šå¤šæ§˜ã«ã™ã‚‹
            temperature=0.7,  # temperatureãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚å°ã•ã„ã»ã©ç¢ºå®šçš„ã«ãªã‚Šã€å¤§ãã„ã»ã©ãƒ©ãƒ³ãƒ€ãƒ ã«ãªã‚Šã¾ã™
            top_p=0.9,  # Top-pã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        )

        # ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦çµæœã‚’å‡ºåŠ›
        response_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)

        print("\nğŸ’¡ ãƒ¢ãƒ‡ãƒ«ã®å›ç­”:")
        print("--------------------")
        print(response_text.strip())
        print("--------------------\n")

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        break

print("ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãŒçµ‚äº†ã—ã¾ã—ãŸã€‚")